# Music Genre Classification

This project builds an **end-to-end Music Genre Classification (MGC)** system using **audio signal processing (MFCC features)** and a **Convolutional Neural Network (CNN)** trained on the **GTZAN** dataset. The trained model is deployed via a **Flask web application**, allowing users to upload audio files and receive **Top-3 genre predictions with confidence scores**.

---

## Project Links

- **Final Report (PDF):**  
  https://drive.google.com/file/d/1yc8d9bEVQ3yn1TJ9WzMl-ZrP3ppq6V0f/view?usp=sharing

- **Presentation Slides:**  
  https://docs.google.com/presentation/d/1tDpHmk3zNLnziO-aKrrh-R4PeRKuX8oQ/edit?usp=sharing&ouid=111650610762216042595&rtpof=true&sd=true

- **Dataset (GTZAN used in this project):**  
  https://drive.google.com/file/d/1dskBzo7LxMXuGnIir9f9zhCyadT9hAlM/view?usp=sharing

- **Project Demo Video:**  
  https://youtu.be/j7CRVgZhN4I

- **Pre-recorded Presentation Video:**  
  https://youtu.be/amlSIy4KrkE

---

## Project Overview

### Objective
Automatically classify music tracks into one of **10 genres**:
**Blues, Classical, Country, Disco, Hip-Hop, Jazz, Metal, Pop, Reggae, Rock**

### Core Techniques
- Audio Signal Processing (MFCC)
- Convolutional Neural Networks (CNN)
- Segment-level prediction aggregation
- Flask-based deployment

---

## Model Pipeline

1. **Audio Input**
   - WAV or MP3 files
   - MP3 files are converted to WAV using FFmpeg

2. **Preprocessing**
   - Resampling to 22,050 Hz
   - Mono conversion
   - Track segmentation (10 segments per 30s track)

3. **Feature Extraction**
   - 13 Mel-Frequency Cepstral Coefficients (MFCCs)
   - FFT window size: 2048
   - Hop length: 512

4. **Deep Learning Model**
   - CNN trained on MFCC feature maps
   - Softmax output for 10 genres

5. **Inference**
   - Predict genre per segment
   - Average probabilities across segments
   - Return Top-3 genres with confidence scores

---

## Folder Structure

```
Music-Genre-Classification/
â”‚
â”œâ”€â”€ app.py                      # Main Flask application
â”œâ”€â”€ app_flask.py                # Alternative Flask version
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # Project documentation
â”‚
â”œâ”€â”€ Data/
â”‚   â””â”€â”€ genres_original/        # GTZAN dataset folders
â”‚
â”œâ”€â”€ templates/                  # HTML templates (Flask/Jinja2)
â”‚   â”œâ”€â”€ base.html
â”‚   â”œâ”€â”€ homepage.html
â”‚   â”œâ”€â”€ prediction.html
â”‚   â”œâ”€â”€ project.html
â”‚   â”œâ”€â”€ About.html
â”‚   â””â”€â”€ contact.html
â”‚
â””â”€â”€ static/
    â”œâ”€â”€ css/
    â”œâ”€â”€ js/
    â””â”€â”€ img/
```

---

## Installation & Setup

### 1. Create Virtual Environment

```
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

### 2. Install Dependencies
```
pip install -r requirements.txt
```

### 3. Install FFmpeg (Required for MP3 support)

```
winget install Gyan.FFmpeg
```

Verify installation:
```
ffmpeg -version
```

---

## Running the Project

From the project root directory:
```
python app.py
```

Open in browser:
```
http://127.0.0.1:5000/
```

---

## Evaluation & Results

The final report includes:
- Confusion Matrix
- ROC Curves and AUC Scores
- Segment-wise Prediction Consistency
- Model Calibration Curves

These analyses demonstrate:
- Genre separability
- Common misclassification patterns
- Reliability of confidence scores

---

## Troubleshooting

- **FFmpeg not found** â†’ Ensure FFmpeg is installed and added to PATH
- **Librosa audio errors** â†’ Reinstall dependencies:
```
pip install --upgrade librosa soundfile
```
- **Templates not rendering** â†’ Ensure `templates/` and `static/` folders exist at root level

---

## ğŸ‘¨â€ğŸ’» Authors

- **Sai Arunanshu Govindarajula**  
  âœ‰ï¸ saiarun@umich.edu

- **Tejaswini**  
  âœ‰ï¸ tejuu@umich.edu
---
