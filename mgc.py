# -*- coding: utf-8 -*-
"""MGC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PW0I4Z0e-yPWVOaBoGh-8oOjQMS4MvWw
"""

import pandas as pd
import json
import os
import math
import numpy as np
import librosa, librosa.display

"""##**READ DATASET**"""

df = pd.read_csv("Data/features_3_sec.csv")
df.head(10)

"""##**ABOUT DATASET**"""

print("Dataset has",df.shape)
print("Count of Positive and Negative samples")
df.label.value_counts().reset_index()

print("Columns with NA values are",list(df.columns[df.isnull().any()]))

df.dtypes

df = df.drop(labels="filename", axis=1)

df.head(10)

"""##**DATA VISUALIZATION**"""

import seaborn as sns
import matplotlib.pyplot as plt
# Computing the Correlation Matrix
spike_cols = [col for col in df.columns if 'mean' in col]
corr = df[spike_cols].corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=np.bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(16, 11));

# Generate a custom diverging colormap
cmap = sns.diverging_palette(0, 25, as_cmap=True, s = 90, l = 45, n = 5)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

plt.title('Correlation Heatmap (for the MEAN variables)', fontsize = 20)
plt.xticks(fontsize = 10)
plt.yticks(fontsize = 10);
plt.savefig("Corr_Heatmap.png")

x = df[["label", "tempo"]]

fig, ax = plt.subplots(figsize=(16, 8));
sns.boxplot(x = "label", y = "tempo", data = x, palette = 'husl');

plt.title('BPM Boxplot for Genres', fontsize = 20)
plt.xticks(fontsize = 14)
plt.yticks(fontsize = 10);
plt.xlabel("Genre", fontsize = 15)
plt.ylabel("BPM", fontsize = 15)
plt.savefig("BPM_Boxplot.png")

import sklearn.preprocessing as skp
data = df.iloc[0:, 1:]
y = data['label']
X = data.loc[:, data.columns != 'label']

# normalize
cols = X.columns
min_max_scaler = skp.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(X)
X = pd.DataFrame(np_scaled, columns = cols)

# Top 2 pca components
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
principalComponents = pca.fit_transform(X)
principalDf = pd.DataFrame(data = principalComponents, columns = ['pc1', 'pc2'])

# concatenate with target label
finalDf = pd.concat([principalDf, y], axis = 1)

plt.figure(figsize = (16, 9))
sns.scatterplot(x = "pc1", y = "pc2", data = finalDf, hue = "label", alpha = 0.7, s = 100);

plt.title('PCA on Genres', fontsize = 20)
plt.xticks(fontsize = 14)
plt.yticks(fontsize = 10);
plt.xlabel("Principal Component 1", fontsize = 15)
plt.ylabel("Principal Component 2", fontsize = 15)
plt.savefig("PCA_Scattert.png")

"""##**BUILDING MODEL**"""

import json
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow.keras as keras

import matplotlib.pyplot as plt
import random

import librosa
import math

DATASET_PATH = "Data/genres_original"
JSON_PATH = "data_10.json"
SAMPLE_RATE = 22050
TRACK_DURATION = 30 # measured in seconds
SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION

import os, json, math
import librosa

def _safe_load(file_path, sr):
    """Load audio safely. Returns (y, sr) or (None, None) if it fails."""
    try:
        y, sr_native = librosa.load(file_path, sr=sr, mono=True)
        if y is None or len(y) == 0:
            raise ValueError("Empty audio")
        return y, sr_native
    except Exception as e:
        print(f"[SKIP] {file_path}\n       {type(e).__name__}: {e}")
        return None, None


def save_mfcc(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=5,
              skip_files=None):
    """
    Extract MFCCs from a music dataset and save them to a JSON file with genre labels.

    dataset_path: root folder containing genre subfolders
    json_path: output JSON path
    skip_files: optional set/list of full paths to skip (normalized)
    """

    if skip_files is None:
        skip_files = set()
    # normalize skip paths for reliable comparison (Windows vs Linux separators)
    skip_files = {os.path.normpath(p) for p in skip_files}

    data = {"mapping": [], "labels": [], "mfcc": []}

    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)
    # librosa frame count can vary by 1 depending on centering/padding; compute a target and accept ±1
    target_frames = int(math.ceil(samples_per_segment / hop_length))

    # Walk the dataset
    for dirpath, dirnames, filenames in os.walk(dataset_path):
        # Only process immediate genre subfolders (not the root itself)
        if os.path.normpath(dirpath) == os.path.normpath(dataset_path):
            continue

        genre = os.path.basename(dirpath)
        if genre not in data["mapping"]:
            data["mapping"].append(genre)

        label = data["mapping"].index(genre)
        print(f"\nProcessing: {genre}  (label={label})")

        for f in filenames:
            if not f.lower().endswith((".wav", ".mp3", ".flac", ".ogg", ".m4a", ".aac")):
                continue

            file_path = os.path.normpath(os.path.join(dirpath, f))
            if file_path in skip_files:
                print(f"[SKIP-MANUAL] {file_path}")
                continue

            signal, sample_rate = _safe_load(file_path, sr=SAMPLE_RATE)
            if signal is None:
                continue

            # process segments
            for d in range(num_segments):
                start = samples_per_segment * d
                finish = start + samples_per_segment

                # skip if segment is too short (end of file issues)
                if finish > len(signal):
                    continue

                mfcc = librosa.feature.mfcc(
                    y=signal[start:finish],
                    sr=sample_rate,
                    n_mfcc=num_mfcc,
                    n_fft=n_fft,
                    hop_length=hop_length
                ).T  # (frames, n_mfcc)

                # accept target_frames ± 1 to avoid dropping tons of samples
                if abs(len(mfcc) - target_frames) <= 1:
                    data["mfcc"].append(mfcc.tolist())
                    data["labels"].append(label)
                    print(f"{file_path}, segment:{d+1}")

    with open(json_path, "w") as fp:
        json.dump(data, fp, indent=4)

    print(f"\nSaved: {json_path}")
    print(f"Total samples: {len(data['mfcc'])}")

save_mfcc(DATASET_PATH, JSON_PATH, num_segments=10)

# path to json
DATA_PATH = "data_10.json"

def load_data(data_path):

    with open(data_path, "r") as f:
        data = json.load(f)

    # convert lists to numpy arrays
    X = np.array(data["mfcc"])
    y = np.array(data["labels"])

    print("Data succesfully loaded!")

    return  X, y

# load data
X, y = load_data(DATA_PATH)

X.shape

# create train, validation and test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

# add an axis to input sets
X_train = X_train[..., np.newaxis]
X_test = X_test[..., np.newaxis]

X_train.shape

input_shape = (X_train.shape[1], X_train.shape[2], 1)

from keras.regularizers import l2
# build the CNN
model_cnn = keras.Sequential()

# 1st conv layer
model_cnn.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))
model_cnn.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))
model_cnn.add(keras.layers.BatchNormalization())


# 2nd conv layer
model_cnn.add(keras.layers.Conv2D(64, (3, 3), activation='relu',kernel_initializer='he_uniform',kernel_regularizer=l2(0.0001)))
model_cnn.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))
model_cnn.add(keras.layers.BatchNormalization())
model_cnn.add(keras.layers.Dropout(0.3))

# 3rd conv layer
model_cnn.add(keras.layers.Conv2D(128, (2, 2), activation='relu',kernel_initializer='he_uniform',kernel_regularizer=l2(0.0001)))
model_cnn.add(keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))
model_cnn.add(keras.layers.BatchNormalization())
model_cnn.add(keras.layers.Dropout(0.3))

# flatten output and feed it into dense layer
model_cnn.add(keras.layers.Flatten())
model_cnn.add(keras.layers.Dense(128, activation='relu',kernel_initializer='he_uniform',kernel_regularizer=l2(0.0001)))
model_cnn.add(keras.layers.BatchNormalization())
model_cnn.add(keras.layers.Dropout(0.3))

# output layer
model_cnn.add(keras.layers.Dense(10, activation='softmax'))

# compile model
optimiser = keras.optimizers.Adam(learning_rate=0.0001)
model_cnn.compile(optimizer=optimiser,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

from tensorflow.keras.callbacks import EarlyStopping
callbacks = [
             EarlyStopping(patience=3)
]

model_cnn.summary()

# train model
history = model_cnn.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=32, epochs=100,callbacks=callbacks)

def plot_history(history):

    fig, axs = plt.subplots(2)

    # create accuracy sublpot
    axs[0].plot(history.history["accuracy"], label="train accuracy")
    axs[0].plot(history.history["val_accuracy"], label="test accuracy")
    axs[0].set_ylabel("Accuracy")
    axs[0].legend(loc="lower right")
    axs[0].set_title("Accuracy eval")

    # create error sublpot
    axs[1].plot(history.history["loss"], label="train error")
    axs[1].plot(history.history["val_loss"], label="test error")
    axs[1].set_ylabel("Error")
    axs[1].set_xlabel("Epoch")
    axs[1].legend(loc="upper right")
    axs[1].set_title("Error eval")

    plt.show()


# plot accuracy and error as a function of the epochs
plot_history(history)

import pandas as pd
metrics_df = pd.DataFrame(history.history)
metrics_df.tail(10)

# evaluate model on Test Set
test_loss, test_acc = model_cnn.evaluate(X_test, y_test, verbose=2)
print('\nTest accuracy:', test_acc)

model_cnn.save("MusicGenre_CNN.h5")

new_model = keras.models.load_model("MusicGenre_CNN.h5")
new_model.evaluate(X_test,y_test,batch_size=128)

"""#**TESTING ON TEST DATASET**"""

# pick a sample to predict from the test set
X_to_predict = X_test[100]
y_to_predict = y_test[100]

X_to_predict.shape

# add a dimension to input data for sample - model.predict() expects a 4d array in this case
X_to_predict = X_to_predict[np.newaxis, ...] # array shape (1, 130, 13, 1)

X_to_predict.shape

# perform prediction
prediction = model_cnn.predict(X_to_predict)

# get index with max value
predicted_index = np.argmax(prediction, axis=1)

print("Predicted Genre:", int(predicted_index))

"""####**ALL in ONE**"""

# pick a sample to predict from the test set
X_to_predict = X_test[50]
y_to_predict = y_test[50]

print("Real Genre:", y_to_predict)

X_to_predict = X_to_predict[np.newaxis, ...]

prediction = model_cnn.predict(X_to_predict)

# get index with max value
predicted_index = np.argmax(prediction, axis=1)

print("Predicted Genre:", int(predicted_index))

"""####**MULTI-TESTING ON TEST DATASET**"""

import random
for n in range(10):

  i = random.randint(0,len(X_test))
  # pick a sample to predict from the test set
  X_to_predict = X_test[i]
  y_to_predict = y_test[i]

  print("\nReal Genre:", y_to_predict)

  X_to_predict = X_to_predict[np.newaxis, ...]

  prediction = model_cnn.predict(X_to_predict)

  # get index with max value
  predicted_index = np.argmax(prediction, axis=1)

  print("Predicted Genre:", int(predicted_index))

"""#**TESTING with EXTERNAL DATA**"""

import librosa
import math
import numpy as np

def process_input(audio_file, track_duration):
    SAMPLE_RATE = 22050
    NUM_MFCC = 13
    N_FFT = 2048
    HOP_LENGTH = 512
    NUM_SEGMENTS = 10

    SAMPLES_PER_TRACK = SAMPLE_RATE * track_duration
    samples_per_segment = int(SAMPLES_PER_TRACK / NUM_SEGMENTS)
    expected_frames = math.ceil(samples_per_segment / HOP_LENGTH)

    signal, sample_rate = librosa.load(audio_file, sr=SAMPLE_RATE, mono=True)

    mfcc_segments = []

    for d in range(NUM_SEGMENTS):
        start = samples_per_segment * d
        finish = start + samples_per_segment

        if finish > len(signal):
            continue

        mfcc = librosa.feature.mfcc(
            y=signal[start:finish],
            sr=sample_rate,
            n_mfcc=NUM_MFCC,
            n_fft=N_FFT,
            hop_length=HOP_LENGTH
        ).T  # (frames, n_mfcc)

        # accept ±1 frame difference (librosa padding behavior)
        if abs(len(mfcc) - expected_frames) <= 1:
            mfcc_segments.append(mfcc)

    return np.array(mfcc_segments)

genre_dict = {0:"disco ",1:"pop",2:"classical",3:"metal",4:"rock",5:"blues",6:"hiphop",7:"reggae",8:"country",9:"jazz"}

new_input_mfcc = process_input("Data/genres_original/classical/classical.00001.wav", 30)

type(new_input_mfcc)

new_input_mfcc.shape

X_to_predict = new_input_mfcc[np.newaxis, ..., np.newaxis]
X_to_predict.shape

print(type(X_to_predict))
try:
    import numpy as np
    print("np.shape:", np.shape(X_to_predict))
    print("dtype:", getattr(X_to_predict, "dtype", None))
except Exception as e:
    print("shape check failed:", e)

X_to_predict = X_to_predict.squeeze(axis=0)
print("Fixed input shape:", X_to_predict.shape)

prediction = model_cnn.predict(X_to_predict, verbose=0)
print("Prediction shape:", prediction.shape)

# Average probabilities across segments
avg_pred = prediction.mean(axis=0)   # shape: (10,)

# Get predicted class index
predicted_index = int(np.argmax(avg_pred))

# Print result
print("Predicted Genre:", genre_dict[predicted_index])

top3 = avg_pred.argsort()[-3:][::-1]

print("Top-3 predictions:")
for idx in top3:
    print(f"{genre_dict[int(idx)]}: {avg_pred[idx]:.4f}")

# Remove extra batch dimension
X_to_predict = X_to_predict.squeeze(axis=0)

print("Fixed input shape:", X_to_predict.shape)
# (10, 130, 13, 1)

# Predict per segment
prediction = model_cnn.predict(X_to_predict)
# prediction shape: (10, num_classes)

# Aggregate segment predictions
avg_pred = np.mean(prediction, axis=0)
predicted_index = int(np.argmax(avg_pred))

print("Predicted Genre:", genre_dict[predicted_index])